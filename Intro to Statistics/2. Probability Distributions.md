# Continuous Probability Distribution
In case of continuous distribution, the probability of any discrete value is 0 (near zero). And the probability of their interval is defined. 
For example, in a 360 degree pendulum, the probability of pendulum being at a specific angle at any time is zero and probability of being in a range is perfectly defined.
## Density
(Probability for Continuous Spaces)
The equation used to describe a continuous probability distribution is called a probability density function. 
Heuristically, the probability density function is just the distribution from which a continuous random variable is drawn, like the normal distribution, which is the PDF of a normally distributed continuous random variable.

> The probability that a random variable X takes a value in the (open or closed) interval [a, b] is given by the integral of a function called **probability density function** f(x):
> $$
P(a <= X <= b) = \int_a^b f(x)dx.
$$
If the random variable can be any real number, the probability density function is normalized so that:
$$
\int_{-\infty}^\infty f(x)dx = 1
$$
[More details](https://brilliant.org/wiki/continuous-random-variables-probability-density/)

Density function can also be non uniform.
Density is the y-axis value of function. Density can be smaller than or equal to or larger than one.
## Correlation and Causation
![Correlation and Causation](https://github.com/kushwahashivam/statistics/blob/master/images/correlation-vs-causation-phone-RAM1.png)
**Correlation** is a statistical technique which tells us how strongly the pair of variables are linearly related and change together. It does not tell us why and how behind the relationship but it just says the relationship exists.
(can be Positive, Negative or No Correlation)
**Example:** Correlation between Ice cream sales and sunglasses sold.

**Causation** takes a step further than correlation. It says any change in the value of one variable will **cause** a change in the value of another variable, which means one variable makes other to happen. It is also referred as cause and effect.

 - Firstly, causation means that two events appear at the same time or one after the other.
 - And secondly, it means these two variables not only appear together, the existence of one causes the other to manifest.
 
**Example:** When a person is exercising then the amount of calories burning goes up every minute. Former is causing latter to happen.

**“Correlation does not imply causation!”**

So the less the information we have the more we are forced to observe correlations. Similarly the more information we have the more transparent things will become and the more we will be able to see the actual casual relationships.

As in summer people usually go out, enjoy nice sunny day and chill themselves with ice creams. So when it’s sunny, wide range of people are outside and there is a wider selection of victims for predators.
![Correlation and Causation](https://github.com/kushwahashivam/statistics/blob/master/images/correlation-vs-causation.png)

[more details](https://towardsdatascience.com/why-correlation-does-not-imply-causation-5b99790df07e)

[Assignment](https://discussions.udacity.com/t/correlation-vs-causation-assignment/186693)
## Estimators
An estimator is a statistic that estimates some fact about the population. You can also think of an estimator as the rule that creates an estimate. For example, the sample mean(x̄) is an estimator for the population mean, μ.

The quantity that is being estimated (i.e. the one you want to know) is called the estimand. For example, let’s say you wanted to know the average height of children in a certain school with a population of 1000 students. You take a sample of 30 children, measure them and find that the mean height is 56 inches. This is your sample mean, the estimator. You use the sample mean to estimate that the population mean (your estimand) is about 56 inches.
## Laplace
Adding fake data to generalize the probability of outcomes.
## Averages
(Mean, Median, Mode)
## Variance
In probability theory and statistics, variance is the expectation of the squared deviation of a random variable from its mean. Informally, it measures how far a set of numbers are spread out from their average value.
The average of the squared differences from the Mean.
Standard Deviation is square root of variance.
**Standard Score** (z-score) is the measure of how far an outcome is from mean. Whether it is one standard deviation or two standard deviation or ... from mean. To calculate this, we calculate the deviation for our outcome and the divide it by standard deviation. It will tell us standard score.

    Standard Score = (X-μ)/σ
## Outliers
Quartiles 
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE0OTk4MDYzODYsLTEzOTM5NTI4ODksMj
EyODI5OTQ3OCwxMDI3Mzc0NTg2LC0yMDIxNTgxOTY3LDk0NDg1
MzE1OSwxNzI3MDA2NzM1LC0yMDg4NzQ2NjEyXX0=
-->